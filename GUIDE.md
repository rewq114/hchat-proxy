# ğŸŒŸ HChat Proxy User Guide

> **Unlock the full potential of HChat with a unified, developer-friendly proxy.**
> Seamlessly connect your favorite AI tools and SDKs to HChat's powerful LLM infrastructure.

---

## âš¡ï¸ Quick Start

**ë‹¨ 3ë‹¨ê³„**ë¡œ HChatì˜ ëª¨ë“  ëª¨ë¸ì„ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ëë‚©ë‹ˆë‹¤.

1.  **ì‹¤í–‰**: Windows ì•±(`HChat Proxy.exe`) ë˜ëŠ” Docker ì»¨í…Œì´ë„ˆë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.
2.  **í™•ì¸**: `http://localhost:11435` ì£¼ì†Œë¡œ ì‹¤í–‰ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤.
3.  **ì—°ê²°**: ì•„ë˜ ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ë¥¼ ì—°ê²°í•˜ì„¸ìš”.

| ì„¤ì • í•­ëª©    | ê°’                          | ì„¤ëª…                    |
| :----------- | :-------------------------- | :---------------------- |
| **Base URL** | `http://localhost:11435/v1` | Proxyì˜ í†µí•© ì—”ë“œí¬ì¸íŠ¸ |
| **API Key**  | `hk-...` (Your HChat Key)   | ê¸°ì¡´ í‚¤ ê·¸ëŒ€ë¡œ ì‚¬ìš©     |

---

## ğŸ”Œ Connection Magic

ëª¨ë“  ì½”ë“œëŠ” **OpenAI í˜¸í™˜ ë°©ì‹**ìœ¼ë¡œ ì‘ì„±í•˜ë©´ ë©ë‹ˆë‹¤. HChat Proxyê°€ ì•Œì•„ì„œ ì¤‘ê³„í•´ ë“œë¦½ë‹ˆë‹¤.

### ğŸ Python SDK

```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:11435/v1",
    api_key="YOUR_HCHAT_KEY"
)

# ğŸ¤– GPT-5 (via Universal Route)
response = client.chat.completions.create(
    model="gpt-5-mini",
    messages=[{"role": "user", "content": "Hello!"}]
)

# ğŸ’ Claude / Gemini (Native Pass-through)
# ëª¨ë¸ ì´ë¦„ë§Œ ë°”ê¾¸ë©´ Proxyê°€ ìë™ìœ¼ë¡œ ë¼ìš°íŒ…í•©ë‹ˆë‹¤!
claude_res = client.chat.completions.create(
    model="claude-sonnet-4-5",
    messages=[{"role": "user", "content": "Hi Claude!"}]
)
```

### ğŸ“¦ Node.js SDK

```javascript
import OpenAI from "openai";

const openai = new OpenAI({
  baseURL: "http://localhost:11435/v1",
  apiKey: "YOUR_HCHAT_KEY",
});

const stream = await openai.chat.completions.create({
  model: "gemini-2.5-pro",
  messages: [{ role: "user", content: "Tell me a story." }],
  stream: true,
});

for await (const chunk of stream) {
  process.stdout.write(chunk.choices[0]?.delta?.content || "");
}
```

### ğŸ§  Thinking Process (CoT)

OpenAI í˜¸í™˜ ëª¨ë“œë¡œ **Claude 3.7**ì´ë‚˜ **Gemini 2.0** ê°™ì€ ì¶”ë¡  ëª¨ë¸ì„ ì‚¬ìš©í•  ê²½ìš°, ëª¨ë¸ì˜ ì‚¬ê³  ê³¼ì •(Chain of Thought)ì€ **`<think>`** íƒœê·¸ì— ê°ì‹¸ì ¸ ì „ë‹¬ë©ë‹ˆë‹¤.

```txt
<think>
ë¬¸ì œë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤...
1. ì…ë ¥ ë°ì´í„° í™•ì¸
2. ë…¼ë¦¬ì  ì¶”ë¡  ìˆ˜í–‰
</think>
ì—¬ê¸°ì— ìµœì¢… ë‹µë³€ì´ ë‚˜ì˜µë‹ˆë‹¤.
```

---

## ğŸ’ Native SDK Support

OpenAI í˜¸í™˜ ë°©ì‹ ì™¸ì—ë„, ê° ì œì¡°ì‚¬ì˜ Native SDKë¥¼ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### ğŸ§  Anthropic (Claude) SDK

**ì£¼ì†Œ(`base_url`)ë§Œ ë³€ê²½**í•˜ë©´ ë©ë‹ˆë‹¤.

#### Python

```python
from anthropic import Anthropic

client = Anthropic(
    base_url="http://localhost:11435/v1",  # /v1/messagesë¡œ ìë™ ë§¤í•‘ë¨
    api_key="YOUR_HCHAT_KEY"
)

message = client.messages.create(
    model="claude-sonnet-4-5",
    max_tokens=1024,
    messages=[{"role": "user", "content": "Hello, Claude!"}]
)
print(message.content[0].text)
```

### âœ¨ Google (Gemini) GenAI SDK

Google ê³µì‹ SDK(`google-genai`)ë¥¼ ì‚¬ìš©í•˜ì—¬ Native ê¸°ëŠ¥ì„ ê·¸ëŒ€ë¡œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### Python

```python
from google import genai

client = genai.Client(
    api_key="YOUR_HCHAT_KEY",
    http_options={'api_endpoint': 'http://localhost:11435/v1'}
)

# 1. Non-streaming
response = client.models.generate_content(
    model='gemini-2.0-flash',
    contents="Explain quantum physics"
)
print(response.text)

# 2. Streaming (New!)
response = client.models.generate_content_stream(
    model='gemini-2.0-flash',
    contents="Tell me a long story"
)
for chunk in response:
    print(chunk.text, end="")
```

---

## ğŸ›  Integration Recipes

ìœ ëª…í•œ AI ë„êµ¬ë“¤ê³¼ë„ ì™„ë²½í•˜ê²Œ í˜¸í™˜ë©ë‹ˆë‹¤.

### ğŸ–±ï¸ Cursor / VSCode (GenAI Plugins)

Cursor ì„¤ì •ì—ì„œ `OpenAI Base URL`ì„ ë®ì–´ì“°ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.

> [!TIP] > **Cursor Settings** -> **Models** -> **OpenAI**
>
> - **Base URL**: `http://localhost:11435/v1`
> - **API Key**: `YOUR_HCHAT_KEY`
> - **Model Name**: `gpt-5-mini` (Add manually)

### ğŸ¦œğŸ”— LangChain

```python
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    base_url="http://localhost:11435/v1",
    api_key="YOUR_HCHAT_KEY",
    model="claude-sonnet-4-5"
)

llm.invoke("What is the capital of HChat?")
```

---

## ğŸ§  Smart Routing Architecture

HChat ProxyëŠ” ëª¨ë¸ ì´ë¦„ì„ ë³´ê³  ê°€ì¥ ì í•©í•œ ê²½ë¡œë¡œ ìš”ì²­ì„ ì „ë‹¬í•©ë‹ˆë‹¤.

```mermaid
flowchart LR
    Client[USER Client]
    Proxy(HChat Proxy)
    Gateway{HChat API}

    Client -->|Req: gpt-5-mini| Proxy
    Client -->|Req: claude-sonnet-4-5| Proxy
    Client -->|Req: gemini-2.5-pro| Proxy

    Proxy -->|Universal Mapper| Gateway
    Proxy -->|Native Transport| Gateway
    Proxy -->|Native Transport| Gateway

    style Proxy fill:#f9f,stroke:#333,stroke-width:2px
    style Gateway fill:#bbf,stroke:#333
```

---

## âš™ï¸ Configuration (Port ë³€ê²½)

ê¸°ë³¸ í¬íŠ¸(`11435`)ê°€ ì¶©ëŒí•˜ê±°ë‚˜ ë³€ê²½í•˜ê³  ì‹¶ë‹¤ë©´:

- **Windows ì•±**: íŠ¸ë ˆì´ ì•„ì´ì½˜ ìš°í´ë¦­ > **Settings** > **Port** ê°’ì„ ìˆ˜ì •í•˜ê³  ì €ì¥í•˜ì„¸ìš”.
- **Docker**: `PORT` í™˜ê²½ë³€ìˆ˜ì™€ `-p` ì˜µì…˜ì„ ë³€ê²½í•˜ì„¸ìš”.
  ```bash
  # ì˜ˆ: ë‚´ë¶€ 3000ë²ˆ í¬íŠ¸ë¡œ ë³€ê²½í•˜ê³  í˜¸ìŠ¤íŠ¸ 8080ì— ë§¤í•‘
  docker run -p 8080:3000 -e PORT=3000 ...
  ```

---

## â“ FAQ

<details>
<summary><strong>Q. Dockerì—ì„œ localhostë¡œ ì ‘ì†ì´ ì•ˆ ë¼ìš”!</strong></summary>

Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œ í˜¸ìŠ¤íŠ¸ì˜ Proxyë¡œ ì ‘ì†í•˜ë ¤ë©´ `host.docker.internal:11435`ë¥¼ ì‚¬ìš©í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (Mac/Windows ê¸°ì¤€)

</details>

<details>
<summary><strong>Q. ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ì€ ì–´ë””ì„œ ë³´ë‚˜ìš”?</strong></summary>

ë¸Œë¼ìš°ì €ì—ì„œ `http://localhost:11435/v1/models` ë¡œ ì ‘ì†í•˜ê±°ë‚˜, ì•±ì˜ **Status Page**ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</details>

---

> [!IMPORTANT] > **ë³´ì•ˆ ì£¼ì˜ì‚¬í•­**: ProxyëŠ” ë¡œì»¬ í™˜ê²½(`localhost`)ì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. ê³µìš© ë„¤íŠ¸ì›Œí¬ì— í¬íŠ¸ë¥¼ ë…¸ì¶œí•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•˜ì„¸ìš”.
